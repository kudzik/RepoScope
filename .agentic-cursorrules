# ðŸ¤– Agentic Cursor Rules - RepoScope
# Strategia optymalizacji usage i kosztÃ³w AI/LLM

## ðŸŽ¯ GÅ‚Ã³wne zasady

### Zasada optymalizacji kosztÃ³w
- **UÅ¼yj najtaÅ„szego dostÄ™pnego modelu do zadania**
- Preferuj modele open-source (Llama, Mistral) gdy to moÅ¼liwe
- UÅ¼ywaj GPT-3.5-turbo zamiast GPT-4 dla prostych zadaÅ„
- Implementuj caching odpowiedzi LLM dla powtarzalnych zapytaÅ„
- Ograniczaj dÅ‚ugoÅ›Ä‡ promptÃ³w i kontekstu do minimum wymaganego

### Strategie optymalizacji usage

#### 1. WybÃ³r modelu na podstawie zadania
```python
def select_optimal_model(task_complexity: str, task_type: str) -> str:
    """
    Wybierz najtaÅ„szy dostÄ™pny model na podstawie zÅ‚oÅ¼onoÅ›ci zadania.

    Args:
        task_complexity: 'simple', 'medium', 'complex'
        task_type: 'analysis', 'generation', 'summarization'

    Returns:
        str: Nazwa modelu do uÅ¼ycia
    """
    if task_complexity == 'simple':
        return 'gpt-3.5-turbo'  # NajtaÅ„szy dla prostych zadaÅ„
    elif task_complexity == 'medium':
        return 'gpt-3.5-turbo'  # Nadal taÅ„szy niÅ¼ GPT-4
    else:  # complex
        return 'gpt-4'  # Tylko dla bardzo zÅ‚oÅ¼onych zadaÅ„
```

#### 2. Caching strategii
```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_llm_response(prompt_hash: str, model: str) -> str:
    """
    Cache odpowiedzi LLM dla powtarzalnych zapytaÅ„.

    Args:
        prompt_hash: Hash promptu dla identyfikacji
        model: Model uÅ¼yty do generowania odpowiedzi

    Returns:
        str: Cached odpowiedÅº
    """
    # Implementacja cache
    pass

def get_prompt_hash(prompt: str) -> str:
    """Generuj hash promptu dla cache."""
    return hashlib.md5(prompt.encode()).hexdigest()
```

#### 3. Optymalizacja promptÃ³w
```python
def optimize_prompt(prompt: str, max_tokens: int = 1000) -> str:
    """
    Optymalizuj prompt do minimum wymaganego.

    Args:
        prompt: Oryginalny prompt
        max_tokens: Maksymalna liczba tokenÃ³w

    Returns:
        str: Zoptymalizowany prompt
    """
    # UsuÅ„ niepotrzebne sÅ‚owa
    # SkrÃ³Ä‡ kontekst do minimum
    # UÅ¼yj bardziej precyzyjnych instrukcji
    return optimized_prompt
```

#### 4. Streaming dla dÅ‚ugich odpowiedzi
```python
async def stream_llm_response(prompt: str, model: str):
    """
    UÅ¼yj streaming API dla dÅ‚ugich odpowiedzi.

    Args:
        prompt: Prompt do przetworzenia
        model: Model do uÅ¼ycia

    Yields:
        str: Fragmenty odpowiedzi
    """
    async for chunk in llm_client.stream(prompt, model=model):
        yield chunk
```

#### 5. Monitoring kosztÃ³w
```python
class CostMonitor:
    """Monitor kosztÃ³w AI/LLM w czasie rzeczywistym."""

    def __init__(self):
        self.usage_stats = {}
        self.cost_threshold = 100.0  # USD

    def track_usage(self, model: str, tokens: int, cost: float):
        """ÅšledÅº uÅ¼ycie i koszty."""
        if model not in self.usage_stats:
            self.usage_stats[model] = {'tokens': 0, 'cost': 0.0}

        self.usage_stats[model]['tokens'] += tokens
        self.usage_stats[model]['cost'] += cost

        if self.usage_stats[model]['cost'] > self.cost_threshold:
            self._alert_high_usage(model)

    def _alert_high_usage(self, model: str):
        """Alert o wysokim uÅ¼yciu."""
        print(f"âš ï¸ High usage detected for {model}")
```

## ðŸ”§ Implementacja w RepoScope

### AnalysisService z optymalizacjÄ… kosztÃ³w
```python
class OptimizedAnalysisService:
    """AnalysisService z optymalizacjÄ… kosztÃ³w AI/LLM."""

    def __init__(self):
        self.cost_monitor = CostMonitor()
        self.llm_client = self._get_cheapest_available_model()
        self.cache = {}

    def _get_cheapest_available_model(self):
        """Wybierz najtaÅ„szy dostÄ™pny model."""
        # Preferuj modele open-source
        if self._is_open_source_available():
            return self._get_open_source_model()

        # Fallback na GPT-3.5-turbo
        return self._get_gpt35_turbo()

    async def analyze_repository_optimized(self, repo_url: str):
        """Zoptymalizowana analiza repozytorium."""
        # 1. UÅ¼yj najtaÅ„szego modelu dla prostych zadaÅ„
        simple_analysis = await self._simple_analysis(repo_url)

        # 2. Cache wynikÃ³w
        cache_key = self._get_cache_key(repo_url)
        if cache_key in self.cache:
            return self.cache[cache_key]

        # 3. UÅ¼yj streaming dla dÅ‚ugich odpowiedzi
        async for chunk in self._stream_analysis(repo_url):
            yield chunk

        # 4. Monitoruj koszty
        self.cost_monitor.track_usage(
            model=self.llm_client.model,
            tokens=len(simple_analysis),
            cost=self._calculate_cost(simple_analysis)
        )
```

## ðŸ“Š Metryki optymalizacji

### Kluczowe wskaÅºniki
- **Cost per request**: Koszt na Å¼Ä…danie
- **Tokens per request**: Tokeny na Å¼Ä…danie
- **Cache hit rate**: WskaÅºnik trafieÅ„ cache
- **Model usage distribution**: RozkÅ‚ad uÅ¼ycia modeli
- **Response time**: Czas odpowiedzi

### Cele optymalizacji
- **Cost reduction**: 50% redukcja kosztÃ³w
- **Cache hit rate**: >80% trafieÅ„ cache
- **Response time**: <2s dla prostych zadaÅ„
- **Token efficiency**: <1000 tokenÃ³w na Å¼Ä…danie

## ðŸš€ Best Practices

### 1. WybÃ³r modelu
- **Proste zadania**: GPT-3.5-turbo
- **Åšrednie zadania**: GPT-3.5-turbo
- **ZÅ‚oÅ¼one zadania**: GPT-4 (tylko gdy konieczne)

### 2. Caching
- Cache odpowiedzi dla powtarzalnych zapytaÅ„
- UÅ¼yj LRU cache z odpowiednim rozmiarem
- Implementuj TTL dla cache

### 3. Prompt optimization
- SkrÃ³Ä‡ prompty do minimum
- UÅ¼yj bardziej precyzyjnych instrukcji
- Unikaj redundantnych informacji

### 4. Streaming
- UÅ¼yj streaming API dla dÅ‚ugich odpowiedzi
- Implementuj chunking dla duÅ¼ych danych
- Monitoruj przepustowoÅ›Ä‡

### 5. Monitoring
- ÅšledÅº koszty w czasie rzeczywistym
- Ustaw alerty dla wysokiego uÅ¼ycia
- Analizuj wzorce uÅ¼ycia

## ðŸ”„ Fallback strategies

### 1. Model fallback
```python
def get_model_with_fallback(task_type: str) -> str:
    """Pobierz model z fallback strategiÄ…."""
    try:
        return get_primary_model(task_type)
    except Exception:
        return get_fallback_model(task_type)
```

### 2. Cost-based fallback
```python
def select_model_by_cost_remaining(budget: float) -> str:
    """Wybierz model na podstawie pozostaÅ‚ego budÅ¼etu."""
    if budget > 50.0:
        return 'gpt-4'
    elif budget > 10.0:
        return 'gpt-3.5-turbo'
    else:
        return 'open-source-model'
```

## ðŸ“ˆ Monitoring i alerting

### 1. Real-time monitoring
```python
class RealTimeMonitor:
    """Monitor w czasie rzeczywistym."""

    def __init__(self):
        self.metrics = {}
        self.alerts = []

    def track_metric(self, metric_name: str, value: float):
        """ÅšledÅº metrykÄ™."""
        self.metrics[metric_name] = value
        self._check_alerts(metric_name, value)

    def _check_alerts(self, metric_name: str, value: float):
        """SprawdÅº alerty."""
        if metric_name == 'cost' and value > 100.0:
            self.alerts.append(f"High cost: {value}")
```

### 2. Cost alerts
```python
def setup_cost_alerts():
    """Skonfiguruj alerty kosztÃ³w."""
    alerts = {
        'daily_cost': 50.0,  # USD
        'monthly_cost': 1000.0,  # USD
        'per_request_cost': 1.0,  # USD
    }
    return alerts
```

## ðŸŽ¯ Implementacja w RepoScope

### 1. AnalysisService optimization
```python
# W backend/services/analysis_service.py
class AnalysisService:
    def __init__(self):
        self.cost_optimizer = CostOptimizer()
        self.llm_client = self._get_optimal_model()

    def _get_optimal_model(self):
        """Wybierz optymalny model."""
        return self.cost_optimizer.select_model('analysis')
```

### 2. Cost optimization middleware
```python
# W backend/middleware/cost_optimization.py
class CostOptimizationMiddleware:
    """Middleware do optymalizacji kosztÃ³w."""

    def __init__(self):
        self.cache = {}
        self.monitor = CostMonitor()

    async def process_request(self, request):
        """PrzetwÃ³rz Å¼Ä…danie z optymalizacjÄ… kosztÃ³w."""
        # Implementacja optymalizacji
        pass
```

### 3. Configuration
```python
# W backend/config/llm_config.py
LLM_CONFIG = {
    'default_model': 'gpt-3.5-turbo',
    'fallback_model': 'gpt-3.5-turbo',
    'max_tokens': 1000,
    'temperature': 0.7,
    'cache_ttl': 3600,  # 1 hour
    'cost_threshold': 100.0,  # USD
}
```

## ðŸ“‹ Checklist optymalizacji

### âœ… Implementacja
- [ ] WybÃ³r najtaÅ„szego modelu do zadania
- [ ] Implementacja cache dla powtarzalnych zapytaÅ„
- [ ] Optymalizacja promptÃ³w do minimum
- [ ] Streaming dla dÅ‚ugich odpowiedzi
- [ ] Monitoring kosztÃ³w w czasie rzeczywistym

### âœ… Monitoring
- [ ] Åšledzenie kosztÃ³w na Å¼Ä…danie
- [ ] Alerty dla wysokiego uÅ¼ycia
- [ ] Analiza wzorcÃ³w uÅ¼ycia
- [ ] Optymalizacja na podstawie danych

### âœ… Fallback
- [ ] Fallback na taÅ„sze modele
- [ ] Graceful degradation
- [ ] Error handling
- [ ] Recovery strategies

## ðŸŽ¯ Cele optymalizacji

### KrÃ³tkoterminowe (1-3 miesiÄ…ce)
- 50% redukcja kosztÃ³w AI/LLM
- 80% cache hit rate
- <2s response time dla prostych zadaÅ„

### DÅ‚ugoterminowe (6-12 miesiÄ™cy)
- 70% redukcja kosztÃ³w AI/LLM
- 90% cache hit rate
- <1s response time dla prostych zadaÅ„
- PeÅ‚na automatyzacja optymalizacji

## ðŸ“š Dokumentacja

### 1. Cost optimization guide
- Strategie optymalizacji kosztÃ³w
- Best practices
- Monitoring i alerting

### 2. Implementation examples
- PrzykÅ‚ady implementacji
- Code snippets
- Configuration templates

### 3. Monitoring dashboard
- Real-time metrics
- Cost tracking
- Usage analytics

---

**Zasada:** UÅ¼yj najtaÅ„szego dostÄ™pnego modelu do zadania - optymalizuj koszty AI/LLM w RepoScope! ðŸš€
