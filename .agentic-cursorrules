# 🤖 Agentic Cursor Rules - RepoScope
# Strategia optymalizacji usage i kosztów AI/LLM

## 🎯 Główne zasady

### Zasada optymalizacji kosztów
- **Użyj najtańszego dostępnego modelu do zadania**
- Preferuj modele open-source (Llama, Mistral) gdy to możliwe
- Używaj GPT-3.5-turbo zamiast GPT-4 dla prostych zadań
- Implementuj caching odpowiedzi LLM dla powtarzalnych zapytań
- Ograniczaj długość promptów i kontekstu do minimum wymaganego

### Strategie optymalizacji usage

#### 1. Wybór modelu na podstawie zadania
```python
def select_optimal_model(task_complexity: str, task_type: str) -> str:
    """
    Wybierz najtańszy dostępny model na podstawie złożoności zadania.

    Args:
        task_complexity: 'simple', 'medium', 'complex'
        task_type: 'analysis', 'generation', 'summarization'

    Returns:
        str: Nazwa modelu do użycia
    """
    if task_complexity == 'simple':
        return 'gpt-3.5-turbo'  # Najtańszy dla prostych zadań
    elif task_complexity == 'medium':
        return 'gpt-3.5-turbo'  # Nadal tańszy niż GPT-4
    else:  # complex
        return 'gpt-4'  # Tylko dla bardzo złożonych zadań
```

#### 2. Caching strategii
```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_llm_response(prompt_hash: str, model: str) -> str:
    """
    Cache odpowiedzi LLM dla powtarzalnych zapytań.

    Args:
        prompt_hash: Hash promptu dla identyfikacji
        model: Model użyty do generowania odpowiedzi

    Returns:
        str: Cached odpowiedź
    """
    # Implementacja cache
    pass

def get_prompt_hash(prompt: str) -> str:
    """Generuj hash promptu dla cache."""
    return hashlib.md5(prompt.encode()).hexdigest()
```

#### 3. Optymalizacja promptów
```python
def optimize_prompt(prompt: str, max_tokens: int = 1000) -> str:
    """
    Optymalizuj prompt do minimum wymaganego.

    Args:
        prompt: Oryginalny prompt
        max_tokens: Maksymalna liczba tokenów

    Returns:
        str: Zoptymalizowany prompt
    """
    # Usuń niepotrzebne słowa
    # Skróć kontekst do minimum
    # Użyj bardziej precyzyjnych instrukcji
    return optimized_prompt
```

#### 4. Streaming dla długich odpowiedzi
```python
async def stream_llm_response(prompt: str, model: str):
    """
    Użyj streaming API dla długich odpowiedzi.

    Args:
        prompt: Prompt do przetworzenia
        model: Model do użycia

    Yields:
        str: Fragmenty odpowiedzi
    """
    async for chunk in llm_client.stream(prompt, model=model):
        yield chunk
```

#### 5. Monitoring kosztów
```python
class CostMonitor:
    """Monitor kosztów AI/LLM w czasie rzeczywistym."""

    def __init__(self):
        self.usage_stats = {}
        self.cost_threshold = 100.0  # USD

    def track_usage(self, model: str, tokens: int, cost: float):
        """Śledź użycie i koszty."""
        if model not in self.usage_stats:
            self.usage_stats[model] = {'tokens': 0, 'cost': 0.0}

        self.usage_stats[model]['tokens'] += tokens
        self.usage_stats[model]['cost'] += cost

        if self.usage_stats[model]['cost'] > self.cost_threshold:
            self._alert_high_usage(model)

    def _alert_high_usage(self, model: str):
        """Alert o wysokim użyciu."""
        print(f"⚠️ High usage detected for {model}")
```

## 🔧 Implementacja w RepoScope

### AnalysisService z optymalizacją kosztów
```python
class OptimizedAnalysisService:
    """AnalysisService z optymalizacją kosztów AI/LLM."""

    def __init__(self):
        self.cost_monitor = CostMonitor()
        self.llm_client = self._get_cheapest_available_model()
        self.cache = {}

    def _get_cheapest_available_model(self):
        """Wybierz najtańszy dostępny model."""
        # Preferuj modele open-source
        if self._is_open_source_available():
            return self._get_open_source_model()

        # Fallback na GPT-3.5-turbo
        return self._get_gpt35_turbo()

    async def analyze_repository_optimized(self, repo_url: str):
        """Zoptymalizowana analiza repozytorium."""
        # 1. Użyj najtańszego modelu dla prostych zadań
        simple_analysis = await self._simple_analysis(repo_url)

        # 2. Cache wyników
        cache_key = self._get_cache_key(repo_url)
        if cache_key in self.cache:
            return self.cache[cache_key]

        # 3. Użyj streaming dla długich odpowiedzi
        async for chunk in self._stream_analysis(repo_url):
            yield chunk

        # 4. Monitoruj koszty
        self.cost_monitor.track_usage(
            model=self.llm_client.model,
            tokens=len(simple_analysis),
            cost=self._calculate_cost(simple_analysis)
        )
```

## 📊 Metryki optymalizacji

### Kluczowe wskaźniki
- **Cost per request**: Koszt na żądanie
- **Tokens per request**: Tokeny na żądanie
- **Cache hit rate**: Wskaźnik trafień cache
- **Model usage distribution**: Rozkład użycia modeli
- **Response time**: Czas odpowiedzi

### Cele optymalizacji
- **Cost reduction**: 50% redukcja kosztów
- **Cache hit rate**: >80% trafień cache
- **Response time**: <2s dla prostych zadań
- **Token efficiency**: <1000 tokenów na żądanie

## 🚀 Best Practices

### 1. Wybór modelu
- **Proste zadania**: GPT-3.5-turbo
- **Średnie zadania**: GPT-3.5-turbo
- **Złożone zadania**: GPT-4 (tylko gdy konieczne)

### 2. Caching
- Cache odpowiedzi dla powtarzalnych zapytań
- Użyj LRU cache z odpowiednim rozmiarem
- Implementuj TTL dla cache

### 3. Prompt optimization
- Skróć prompty do minimum
- Użyj bardziej precyzyjnych instrukcji
- Unikaj redundantnych informacji

### 4. Streaming
- Użyj streaming API dla długich odpowiedzi
- Implementuj chunking dla dużych danych
- Monitoruj przepustowość

### 5. Monitoring
- Śledź koszty w czasie rzeczywistym
- Ustaw alerty dla wysokiego użycia
- Analizuj wzorce użycia

## 🔄 Fallback strategies

### 1. Model fallback
```python
def get_model_with_fallback(task_type: str) -> str:
    """Pobierz model z fallback strategią."""
    try:
        return get_primary_model(task_type)
    except Exception:
        return get_fallback_model(task_type)
```

### 2. Cost-based fallback
```python
def select_model_by_cost_remaining(budget: float) -> str:
    """Wybierz model na podstawie pozostałego budżetu."""
    if budget > 50.0:
        return 'gpt-4'
    elif budget > 10.0:
        return 'gpt-3.5-turbo'
    else:
        return 'open-source-model'
```

## 📈 Monitoring i alerting

### 1. Real-time monitoring
```python
class RealTimeMonitor:
    """Monitor w czasie rzeczywistym."""

    def __init__(self):
        self.metrics = {}
        self.alerts = []

    def track_metric(self, metric_name: str, value: float):
        """Śledź metrykę."""
        self.metrics[metric_name] = value
        self._check_alerts(metric_name, value)

    def _check_alerts(self, metric_name: str, value: float):
        """Sprawdź alerty."""
        if metric_name == 'cost' and value > 100.0:
            self.alerts.append(f"High cost: {value}")
```

### 2. Cost alerts
```python
def setup_cost_alerts():
    """Skonfiguruj alerty kosztów."""
    alerts = {
        'daily_cost': 50.0,  # USD
        'monthly_cost': 1000.0,  # USD
        'per_request_cost': 1.0,  # USD
    }
    return alerts
```

## 🎯 Implementacja w RepoScope

### 1. AnalysisService optimization
```python
# W backend/services/analysis_service.py
class AnalysisService:
    def __init__(self):
        self.cost_optimizer = CostOptimizer()
        self.llm_client = self._get_optimal_model()

    def _get_optimal_model(self):
        """Wybierz optymalny model."""
        return self.cost_optimizer.select_model('analysis')
```

### 2. Cost optimization middleware
```python
# W backend/middleware/cost_optimization.py
class CostOptimizationMiddleware:
    """Middleware do optymalizacji kosztów."""

    def __init__(self):
        self.cache = {}
        self.monitor = CostMonitor()

    async def process_request(self, request):
        """Przetwórz żądanie z optymalizacją kosztów."""
        # Implementacja optymalizacji
        pass
```

### 3. Configuration
```python
# W backend/config/llm_config.py
LLM_CONFIG = {
    'default_model': 'gpt-3.5-turbo',
    'fallback_model': 'gpt-3.5-turbo',
    'max_tokens': 1000,
    'temperature': 0.7,
    'cache_ttl': 3600,  # 1 hour
    'cost_threshold': 100.0,  # USD
}
```

## 📋 Checklist optymalizacji

### ✅ Implementacja
- [ ] Wybór najtańszego modelu do zadania
- [ ] Implementacja cache dla powtarzalnych zapytań
- [ ] Optymalizacja promptów do minimum
- [ ] Streaming dla długich odpowiedzi
- [ ] Monitoring kosztów w czasie rzeczywistym

### ✅ Monitoring
- [ ] Śledzenie kosztów na żądanie
- [ ] Alerty dla wysokiego użycia
- [ ] Analiza wzorców użycia
- [ ] Optymalizacja na podstawie danych

### ✅ Fallback
- [ ] Fallback na tańsze modele
- [ ] Graceful degradation
- [ ] Error handling
- [ ] Recovery strategies

## 🎯 Cele optymalizacji

### Krótkoterminowe (1-3 miesiące)
- 50% redukcja kosztów AI/LLM
- 80% cache hit rate
- <2s response time dla prostych zadań

### Długoterminowe (6-12 miesięcy)
- 70% redukcja kosztów AI/LLM
- 90% cache hit rate
- <1s response time dla prostych zadań
- Pełna automatyzacja optymalizacji

## 📚 Dokumentacja

### 1. Cost optimization guide
- Strategie optymalizacji kosztów
- Best practices
- Monitoring i alerting

### 2. Implementation examples
- Przykłady implementacji
- Code snippets
- Configuration templates

### 3. Monitoring dashboard
- Real-time metrics
- Cost tracking
- Usage analytics

---

**Zasada:** Użyj najtańszego dostępnego modelu do zadania - optymalizuj koszty AI/LLM w RepoScope! 🚀
