# ðŸ¤– Optymalizacja kosztÃ³w AI/LLM - RepoScope

## ðŸ“‹ PrzeglÄ…d

Dokumentacja strategii optymalizacji kosztÃ³w AI/LLM w RepoScope zgodnie z zasadÄ… **"UÅ¼yj najtaÅ„szego dostÄ™pnego modelu do zadania"**.

## ðŸŽ¯ GÅ‚Ã³wne zasady

### 1. Zasada optymalizacji kosztÃ³w

- **UÅ¼yj najtaÅ„szego dostÄ™pnego modelu do zadania**
- Preferuj modele open-source (Llama, Mistral) gdy to moÅ¼liwe
- UÅ¼ywaj GPT-3.5-turbo zamiast GPT-4 dla prostych zadaÅ„
- Implementuj caching odpowiedzi LLM dla powtarzalnych zapytaÅ„
- Ograniczaj dÅ‚ugoÅ›Ä‡ promptÃ³w i kontekstu do minimum wymaganego

### 2. Strategie optymalizacji

#### WybÃ³r modelu na podstawie zadania

```python
def select_optimal_model(task_complexity: str, task_type: str) -> str:
    """
    Wybierz najtaÅ„szy dostÄ™pny model na podstawie zÅ‚oÅ¼onoÅ›ci zadania.

    Args:
        task_complexity: 'simple', 'medium', 'complex'
        task_type: 'analysis', 'generation', 'summarization'

    Returns:
        str: Nazwa modelu do uÅ¼ycia
    """
    if task_complexity == 'simple':
        return 'gpt-3.5-turbo'  # NajtaÅ„szy dla prostych zadaÅ„
    elif task_complexity == 'medium':
        return 'gpt-3.5-turbo'  # Nadal taÅ„szy niÅ¼ GPT-4
    else:  # complex
        return 'gpt-4'  # Tylko dla bardzo zÅ‚oÅ¼onych zadaÅ„
```

#### Caching strategii

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_llm_response(prompt_hash: str, model: str) -> str:
    """
    Cache odpowiedzi LLM dla powtarzalnych zapytaÅ„.

    Args:
        prompt_hash: Hash promptu dla identyfikacji
        model: Model uÅ¼yty do generowania odpowiedzi

    Returns:
        str: Cached odpowiedÅº
    """
    # Implementacja cache
    pass

def get_prompt_hash(prompt: str) -> str:
    """Generuj hash promptu dla cache."""
    return hashlib.md5(prompt.encode()).hexdigest()
```

#### Optymalizacja promptÃ³w

```python
def optimize_prompt(prompt: str, max_tokens: int = 1000) -> str:
    """
    Optymalizuj prompt do minimum wymaganego.

    Args:
        prompt: Oryginalny prompt
        max_tokens: Maksymalna liczba tokenÃ³w

    Returns:
        str: Zoptymalizowany prompt
    """
    # UsuÅ„ niepotrzebne sÅ‚owa
    # SkrÃ³Ä‡ kontekst do minimum
    # UÅ¼yj bardziej precyzyjnych instrukcji
    return optimized_prompt
```

## ðŸ—ï¸ Architektura optymalizacji

### Komponenty systemu

#### 1. LLMOptimizationConfig

```python
class LLMOptimizationConfig:
    """Configuration for LLM cost optimization."""

    def __init__(self):
        self.model_costs = {
            "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
            "gpt-4": {"input": 0.03, "output": 0.06},
            "claude-3-haiku": {"input": 0.00025, "output": 0.00125},
        }

        self.task_model_mapping = {
            TaskComplexity.SIMPLE: ["gpt-3.5-turbo", "claude-3-haiku"],
            TaskComplexity.MEDIUM: ["gpt-3.5-turbo", "gpt-4-turbo"],
            TaskComplexity.COMPLEX: ["gpt-4", "claude-3-opus"],
        }
```

#### 2. CostMonitor

```python
class CostMonitor:
    """Monitor and track LLM usage costs."""

    def track_usage(self, model: str, input_tokens: int,
                   output_tokens: int, cost: float):
        """Track model usage and costs."""
        # Update daily/monthly stats
        # Check for alerts
        # Monitor per-model usage
```

#### 3. ResponseCache

```python
class ResponseCache:
    """Cache for LLM responses to reduce costs."""

    def get(self, prompt: str, model: str) -> Optional[str]:
        """Get cached response if available and not expired."""

    def set(self, prompt: str, model: str, response: str):
        """Cache a response."""
```

#### 4. CostOptimizationMiddleware

```python
class CostOptimizationMiddleware:
    """Middleware for optimizing LLM costs."""

    async def process_request(self, prompt: str, task_complexity: TaskComplexity):
        """Process a request with cost optimization."""
        # 1. Check cache first
        # 2. Select optimal model
        # 3. Estimate cost
        # 4. Check cost limits
        # 5. Process request
        # 6. Track usage
        # 7. Cache response
```

## ðŸ“Š Metryki optymalizacji

### Kluczowe wskaÅºniki

- **Cost per request**: Koszt na Å¼Ä…danie
- **Tokens per request**: Tokeny na Å¼Ä…danie
- **Cache hit rate**: WskaÅºnik trafieÅ„ cache
- **Model usage distribution**: RozkÅ‚ad uÅ¼ycia modeli
- **Response time**: Czas odpowiedzi

### Cele optymalizacji

- **Cost reduction**: 50% redukcja kosztÃ³w
- **Cache hit rate**: >80% trafieÅ„ cache
- **Response time**: <2s dla prostych zadaÅ„
- **Token efficiency**: <1000 tokenÃ³w na Å¼Ä…danie

## ðŸš€ Best Practices

### 1. WybÃ³r modelu

- **Proste zadania**: GPT-3.5-turbo
- **Åšrednie zadania**: GPT-3.5-turbo
- **ZÅ‚oÅ¼one zadania**: GPT-4 (tylko gdy konieczne)

### 2. Caching

- Cache odpowiedzi dla powtarzalnych zapytaÅ„
- UÅ¼yj LRU cache z odpowiednim rozmiarem
- Implementuj TTL dla cache

### 3. Prompt optimization

- SkrÃ³Ä‡ prompty do minimum
- UÅ¼yj bardziej precyzyjnych instrukcji
- Unikaj redundantnych informacji

### 4. Streaming

- UÅ¼yj streaming API dla dÅ‚ugich odpowiedzi
- Implementuj chunking dla duÅ¼ych danych
- Monitoruj przepustowoÅ›Ä‡

### 5. Monitoring

- ÅšledÅº koszty w czasie rzeczywistym
- Ustaw alerty dla wysokiego uÅ¼ycia
- Analizuj wzorce uÅ¼ycia

## ðŸ”„ Fallback strategies

### 1. Model fallback

```python
def get_model_with_fallback(task_type: str) -> str:
    """Pobierz model z fallback strategiÄ…."""
    try:
        return get_primary_model(task_type)
    except Exception:
        return get_fallback_model(task_type)
```

### 2. Cost-based fallback

```python
def select_model_by_cost_remaining(budget: float) -> str:
    """Wybierz model na podstawie pozostaÅ‚ego budÅ¼etu."""
    if budget > 50.0:
        return 'gpt-4'
    elif budget > 10.0:
        return 'gpt-3.5-turbo'
    else:
        return 'open-source-model'
```

## ðŸ“ˆ Monitoring i alerting

### 1. Real-time monitoring

```python
class RealTimeMonitor:
    """Monitor w czasie rzeczywistym."""

    def track_metric(self, metric_name: str, value: float):
        """ÅšledÅº metrykÄ™."""
        self.metrics[metric_name] = value
        self._check_alerts(metric_name, value)
```

### 2. Cost alerts

```python
def setup_cost_alerts():
    """Skonfiguruj alerty kosztÃ³w."""
    alerts = {
        'daily_cost': 50.0,  # USD
        'monthly_cost': 1000.0,  # USD
        'per_request_cost': 1.0,  # USD
    }
    return alerts
```

## ðŸŽ¯ Implementacja w RepoScope

### 1. AnalysisService optimization

```python
# W backend/services/analysis_service.py
class AnalysisService:
    def __init__(self):
        self.cost_optimizer = CostOptimizer()
        self.llm_client = self._get_optimal_model()

    def _get_optimal_model(self):
        """Wybierz optymalny model."""
        return self.cost_optimizer.select_model('analysis')
```

### 2. Cost optimization middleware

```python
# W backend/middleware/cost_optimization.py
class CostOptimizationMiddleware:
    """Middleware do optymalizacji kosztÃ³w."""

    def __init__(self):
        self.cache = {}
        self.monitor = CostMonitor()

    async def process_request(self, request):
        """PrzetwÃ³rz Å¼Ä…danie z optymalizacjÄ… kosztÃ³w."""
        # Implementacja optymalizacji
        pass
```

### 3. Configuration

```python
# W backend/config/llm_config.py
LLM_CONFIG = {
    'default_model': 'gpt-3.5-turbo',
    'fallback_model': 'gpt-3.5-turbo',
    'max_tokens': 1000,
    'temperature': 0.7,
    'cache_ttl': 3600,  # 1 hour
    'cost_threshold': 100.0,  # USD
}
```

## ðŸ“‹ Checklist optymalizacji

### âœ… Implementacja

- [ ] WybÃ³r najtaÅ„szego modelu do zadania
- [ ] Implementacja cache dla powtarzalnych zapytaÅ„
- [ ] Optymalizacja promptÃ³w do minimum
- [ ] Streaming dla dÅ‚ugich odpowiedzi
- [ ] Monitoring kosztÃ³w w czasie rzeczywistym

### âœ… Monitoring

- [ ] Åšledzenie kosztÃ³w na Å¼Ä…danie
- [ ] Alerty dla wysokiego uÅ¼ycia
- [ ] Analiza wzorcÃ³w uÅ¼ycia
- [ ] Optymalizacja na podstawie danych

### âœ… Fallback

- [ ] Fallback na taÅ„sze modele
- [ ] Graceful degradation
- [ ] Error handling
- [ ] Recovery strategies

## ðŸŽ¯ Cele optymalizacji

### KrÃ³tkoterminowe (1-3 miesiÄ…ce)

- 50% redukcja kosztÃ³w AI/LLM
- 80% cache hit rate
- <2s response time dla prostych zadaÅ„

### DÅ‚ugoterminowe (6-12 miesiÄ™cy)

- 70% redukcja kosztÃ³w AI/LLM
- 90% cache hit rate
- <1s response time dla prostych zadaÅ„
- PeÅ‚na automatyzacja optymalizacji

## ðŸ“š Dokumentacja

### 1. Cost optimization guide

- Strategie optymalizacji kosztÃ³w
- Best practices
- Monitoring i alerting

### 2. Implementation examples

- PrzykÅ‚ady implementacji
- Code snippets
- Configuration templates

### 3. Monitoring dashboard

- Real-time metrics
- Cost tracking
- Usage analytics

## ðŸ”§ Uruchamianie testÃ³w

```bash
# Testy optymalizacji kosztÃ³w
cd backend
python -m pytest tests/test_cost_optimization.py -v

# Testy z coverage
python -m pytest tests/test_cost_optimization.py --cov=config.llm_optimization --cov=middleware.cost_optimization
```

## ðŸ“Š PrzykÅ‚ady uÅ¼ycia

### 1. Podstawowe uÅ¼ycie

```python
from middleware.cost_optimization import cost_optimization_middleware
from config.llm_optimization import TaskComplexity

# Procesuj Å¼Ä…danie z optymalizacjÄ…
result = await cost_optimization_middleware.process_request(
    prompt="Analyze this repository",
    task_complexity=TaskComplexity.SIMPLE
)

print(f"Response: {result['response']}")
print(f"Cost: ${result['cost']:.4f}")
print(f"Model: {result['model']}")
```

### 2. Monitoring kosztÃ³w

```python
# Pobierz statystyki optymalizacji
stats = cost_optimization_middleware.get_optimization_stats()

print(f"Daily cost: ${stats['usage_stats']['daily']['cost']:.2f}")
print(f"Cache hit rate: {stats['cache_stats']['size']}")
print(f"Alerts: {len(stats['alerts'])}")
```

### 3. Konfiguracja modeli

```python
from config.llm_optimization import llm_config, TaskComplexity

# Wybierz optymalny model
model = llm_config.get_optimal_model(TaskComplexity.SIMPLE)
print(f"Optimal model: {model}")

# SprawdÅº koszt
cost = llm_config.get_cost_estimate("gpt-3.5-turbo", 1000, 500)
print(f"Estimated cost: ${cost:.4f}")
```

---

**Zasada:** UÅ¼yj najtaÅ„szego dostÄ™pnego modelu do zadania - optymalizuj koszty AI/LLM w RepoScope! ðŸš€
